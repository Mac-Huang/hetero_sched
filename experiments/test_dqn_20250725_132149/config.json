{
  "experiment_name": "test_dqn",
  "description": "Deep RL for heterogeneous task scheduling",
  "tags": [
    "deep_rl",
    "scheduling",
    "multi_objective"
  ],
  "env_config": "default",
  "max_episode_steps": 50,
  "reward_strategy": "adaptive",
  "reward_weights": {
    "latency": 0.3,
    "energy": 0.2,
    "throughput": 0.25,
    "fairness": 0.15,
    "stability": 0.1
  },
  "agent_type": "DQN",
  "learning_rate": 0.0001,
  "batch_size": 16,
  "gamma": 0.99,
  "tau": 0.001,
  "buffer_size": 100000,
  "min_buffer_size": 30,
  "epsilon_start": 1.0,
  "epsilon_end": 0.01,
  "epsilon_decay": 0.995,
  "total_episodes": 30,
  "max_steps_per_episode": 1000,
  "update_frequency": 4,
  "target_update_frequency": 1000,
  "eval_frequency": 15,
  "save_frequency": 500,
  "log_frequency": 10,
  "early_stopping_patience": 200,
  "min_improvement_threshold": 0.01,
  "target_reward_threshold": 50.0,
  "device": "cuda",
  "num_workers": 1,
  "seed": 42,
  "log_dir": "logs",
  "model_dir": "models",
  "results_dir": "results",
  "tensorboard_log": true,
  "verbose": 1
}